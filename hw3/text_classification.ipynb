{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hamzaliaqet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hamzaliaqet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hamzaliaqet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Classification [30pts]\n",
    "In this problem (again!), you will be analyzing the Twitter data we extracted using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "For every tweet, we collected two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "We divided the tweets into three parts - train, test and hidden test - the first two of which are available to you in CSV files. For train, both the `screen_name` and `text` attributes were provided but for test, `screen_name` was hidden.\n",
    "\n",
    "The overarching goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (i.e., true class labels) is determined from the `screen_name` of the tweet as follows\n",
    "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
    "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
    "\n",
    "Thus, this is a binary classification problem. \n",
    "\n",
    "The problem proceeds in three stages:\n",
    "1. **Text processing (8pts)**: We will clean up the raw tweet text using the various functions offered by the [nltk](http://www.nltk.org/genindex.html) package.\n",
    "2. **Feature construction (10pts)**: In this part, we will construct bag-of-words feature vectors and training labels from the processed text of tweets and the `screen_name` columns respectively.\n",
    "3. **Classification (12pts)**: Using the features derived, we will use [sklearn](http://scikit-learn.org/stable/modules/classes.html) package to learn a model which classifies the tweets as desired. \n",
    "\n",
    "As mentioned earlier, you will use two new python packages in this problem: `nltk` and `sklearn`, both of which should be available with anaconda. However, NLTK comes with many corpora, toy grammars, trained models, etc, which have to be downloaded manually. This assignment requires NLTK's stopwords list and WordNetLemmatizer. Install them using:\n",
    "\n",
    "  ```python\n",
    "  >>>nltk.download('stopwords')\n",
    "  >>>nltk.download('wordnet')\n",
    "  ```\n",
    "\n",
    "Verify that the following commands work for you, before moving on.\n",
    "\n",
    "  ```python\n",
    "  >>>lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "  >>>stopwords=nltk.corpus.stopwords.words('english')\n",
    "  ```\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Text Processing [6pts + 2pts]\n",
    "\n",
    "You first task to fill in the following function which processes and tokenizes raw text. The generated list of tokens should meet the following specifications:\n",
    "1. The tokens must all be in lower case.\n",
    "2. The tokens should appear in the same order as in the raw text.\n",
    "3. The tokens must be in their lemmatized form. If a word cannot be lemmatized (i.e, you get an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
    "4. The tokens must not contain any punctuations. Punctuations should be handled as follows: (a) Apostrophe of the form `'s` must be ignored. e.g., `She's` becomes `she`. (b) Other apostrophes should be omitted. e.g, `don't` becomes `dont`. (c) Words must be broken at the hyphen and other punctuations. \n",
    "\n",
    "Part of your work is to figure out a logical order to carry out the above operations. You may find `string.punctuation` useful, to get hold of all punctuation symbols. Your tokens must be of type `str`. Use `nltk.word_tokenize()` for tokenization once you have handled punctuation in the manner specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def omit_punc(text_list):\n",
    "    if len(text_list) < 1:\n",
    "        return []\n",
    "    new_word = [text_list[0]]\n",
    "    if any(map(lambda x : x in string.punctuation, text_list[0])):\n",
    "        word_len = len(text_list[0])\n",
    "        if text_list[0][word_len - 2] == \"'\":\n",
    "            if text_list[0][word_len - 1] == \"s\":\n",
    "                new_word = text_list[0][:word_len - 2]\n",
    "                new_word = re.split(r\"[\\W]\", new_word) \n",
    "            else:\n",
    "                new_word = text_list[0][:word_len - 2] + text_list[0][word_len - 1]\n",
    "                new_word = re.split(r\"[\\W]\", new_word) \n",
    "        else:\n",
    "            new_word = re.split(r\"[\\W]\", text_list[0])  \n",
    "\n",
    "    new_word = list(filter(None, new_word)) ## removes empty strs \n",
    "    return new_word + omit_punc(text_list[1:])\n",
    "        \n",
    "    \n",
    "\n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    text_split = text_lower.split()\n",
    "    lemmatized_and_lower = []\n",
    "    for row in text_split:\n",
    "        try:\n",
    "            lemmatized_and_lower.append(lemmatizer.lemmatize(row))\n",
    "        except:\n",
    "            ignore = lemmatized_and_lower.append(lemmatizer.lemmatize(row))       \n",
    "            \n",
    "    no_punc = omit_punc(lemmatized_and_lower)\n",
    "    return no_punc\n",
    "\n",
    "    \n",
    "# def main():\n",
    "#     sen = \"i.dsf.s.f.mKainessdf's ðŸ˜‚ðŸ˜‚ .fdslfsl....guid....ing principle: the belief that you can make a difference through public service\"\n",
    "#     print(process(sen, lemmatizer))\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the above function as follows. Try to make your test strings as exhaustive as possible. Some checks are:\n",
    "    \n",
    "   ```python\n",
    "   >>> process(\"I'm doing well! How about you?\")\n",
    "   ['im', 'doing', 'well', 'how', 'about', 'you']\n",
    "   ```\n",
    "\n",
    "   ```python\n",
    "   >>> process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\")\n",
    "   ['education', 'is', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'losing', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "text = \"This is a sample test input processing.\"\n",
    "print(process(text))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "print(tweets.head())\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use the `process()` function we implemented to convert the pandas dataframe we just loaded from tweets_train.csv file. Your function should be able to handle any data frame which contains a column called `text`. The data frame you return should replace every string in `text` with the result of `process()` and retain all other columns as such. Do not change the order of rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recursive function is very natural. \n",
    "\n",
    "\n",
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process_text() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process_text() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "    for i in range(len(df)):\n",
    "        df[\"text\"][i] = process(df[\"text\"][i], lemmatizer)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "# AUTOLAB_IGNORE_START\n",
    "processed_tweets = process_all(tweets)\n",
    "\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "   ```python\n",
    "    >>> print processed_tweets.head()\n",
    "          screen_name                                               text\n",
    "    0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
    "    1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, is, cl...\n",
    "    2  HillaryClinton  [trump, calling, for, trillion, dollar, tax, c...\n",
    "    3  HillaryClinton  [timkaine, guiding, principle, the, belief, th...\n",
    "    4        timkaine  [glad, the, senate, could, pas, a, thud, milco...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Construction [4pts + 4pts + 2pts]\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words TF-IDF feature vector.\n",
    "\n",
    "But before that, as you may have guessed, the number of possible words is prohibitively large and not all of them may be useful for our classification task. Our first sub-task is to determine which words to retain, and which to omit. The common heuristic is to construct a frequency distribution of words in the corpus and prune out the head and tail of the distribution. The intuition of the above operation is as follows. Very common words (i.e. stopwords) add almost no information regarding similarity of two pieces of text. Conversely, very rare words tend to be typos. \n",
    "\n",
    "As NLTK has a list of in-built stop words which is a good substitute for head of the distribution, we will now implement a function which identifies rare words (tail). We will consider a word rare if it occurs not more than once in whole of tweets_train.csv.\n",
    "\n",
    "Using `collections.Counter` will make your life easier.\n",
    "   ```python\n",
    "   >>> Counter(['sample', 'test', 'input', 'processing', 'sample'])\n",
    "    Counter({'input': 1, 'processing': 1, 'sample': 2, 'test': 1})\n",
    "   ```\n",
    "For details on other operations you can perform with Counter, see [this](https://docs.python.org/2/library/collections.html#collections.Counter) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_words(processed_tweets):\n",
    "    \"\"\" use the word count information across all tweets in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: the output of process_all() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    ## I have about 600 words : 1 that occured more than once. \n",
    "    all_rare_words_Counter = Counter()\n",
    "    count = 0\n",
    "    for row in processed_tweets[\"text\"]:\n",
    "           \n",
    "            count_of_rare_words_in_1_list = Counter(row)\n",
    "            count = len(count_of_rare_words_in_1_list)\n",
    "            all_rare_words_Counter.update(count_of_rare_words_in_1_list)\n",
    "    print(len(all_rare_words_Counter))\n",
    "    rare_words = []\n",
    "    for key, val in all_rare_words_Counter.items():\n",
    "        if val == 1:\n",
    "            rare_words.append(key)\n",
    "    return rare_words\n",
    "\n",
    "\n",
    "# # AUTOLAB_IGNORE_START\n",
    "rare_words = get_rare_words(processed_tweets)\n",
    "print((rare_words)) # should give 21280\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a sparse matrix of features for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer`. Remember to ignore the rare words obtained above and NLTK's stop words during the feature creation step. You must leave other optional parameters (e.g., `vocab`, `norm`, etc) at their default values. Is the number of features returned by `TfidfVectorizer` lesser than expected? Any idea why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(processed_tweets, rare_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'\n",
    "        rare_words: list(str): one of the outputs of get_feature_and_rare_words() function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "                                                we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    stop_and_rare_words = stopwords + rare_words\n",
    "    tfidf = sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stop_and_rare_words)\n",
    "    \n",
    "    list_of_all_words_in_processed_tweets = []\n",
    "    count = 0\n",
    "    for row in processed_tweets[\"text\"]:\n",
    "        count = count + 1\n",
    "        doc = \"\"\n",
    "        for word in row:\n",
    "            doc = doc + word + \" \"\n",
    "        list_of_all_words_in_processed_tweets.append(doc)\n",
    "    sp = tfidf.fit_transform(list_of_all_words_in_processed_tweets)\n",
    "    return tfidf, sp\n",
    "    \n",
    "    \n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "(tfidf, X) = create_features(processed_tweets, rare_words)\n",
    "# print(X.shape)   ## [n_samples, n_features]\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for each tweet, assign a class label (0 or 1) using its `screen_name`. Use 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from screen_name\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    labels = np.array([], dtype=int)\n",
    "    for row in processed_tweets[\"screen_name\"]:\n",
    "        if row == \"realDonaldTrump\" or row == \"mike_pence\" or row == \"GOP\":\n",
    "            labels = np.append(labels, 0)\n",
    "        else:\n",
    "            labels = np.append(labels, 1)\n",
    "    return labels\n",
    "# AUTOLAB_IGNORE_START\n",
    "labels = create_labels(processed_tweets)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification [4pts + 4pts + 4pts]\n",
    "And finally, we are ready to put things together and learn a model for the classification of tweets. The classifier you will be using is [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (Support Vector Machine). [Here](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html) is a nice introduction to SVMs for the curious minded. (But we will cover it in the class soon!)\n",
    "\n",
    "At the heart of SVMs is the concept of kernel functions, which determines how the similarity/distance between two data points in computed. `sklearn`'s SVM provides four kernel functions: `linear`, `poly`, `rbf`, `sigmoid` (details [here](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)) but you can also implement your own distance function and pass it as an argument to the classifier.\n",
    "\n",
    "Through the various functions you implement in this part, you will be able to learn a classifier, score a classifier based on how well it performs and use it for prediction tasks.\n",
    "\n",
    "Specifically, you will carry out the following tasks in order:\n",
    "1. Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. Stick to default values for any other optional parameters.\n",
    "2. Implement the `evaluate_classifier()` function which scores a classifier based on accuracy.\n",
    "3. Call `learn_classifier()` and `evaluate_classifier()` for each of the four kernel modes and determine what performs the best (this code has been written for you already).\n",
    "4. Go back to `learn_classifier()` and fill in the best mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_classifier(X_train, y_train, kernel='best'):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features_and_labels()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_features_and_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [best|linear|poly|rbf|sigmoid]\n",
    "                    if 'best' is supplied, reset the kernel parameter to the value you have determined to be the best\n",
    "    Outputs:\n",
    "        sklearn.svm.classes.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    if kernel == 'best':\n",
    "        kernel = 'linear' # fill the best mode after you finish the evaluate_classifier() function\n",
    "    \n",
    "    clf = sklearn.svm.SVC(C=1e4, kernel=kernel)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "# classifier = learn_classifier(X, y, kernel)\n",
    "# classifier\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to learn a classifier, the next step is to evaluate it, ie., characterize how good its classification performance is. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n",
    "\n",
    "There are two questions that should now come to your mind:\n",
    "1. **What data to use?** The data used to evaluate a classifier is called **validation data**, and it is usually different from the data used for training (for reasons we will learn about in the later lectures). However, in this problem, you will use training data as the validation data as well.\n",
    "\n",
    "2. **And what metric?** There are several evaluation measures available in the literature (e.g., accuracy, precision, recall, F-1,etc) and different fields have different preferences for specific metrics due to different goals. We will go with accuracy. According to wiki, **accuracy** of a classifier measures the fraction of all data points that are correctly classified by it; it is the ratio of the number of correct classifications to the total number of (correct or incorrect) classifications.\n",
    "\n",
    "Now, implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    predicted_labels = classifier.predict(X_validation)\n",
    "    wrong_predictions = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != predicted_labels[i]:\n",
    "            wrong_predictions = wrong_predictions + 1\n",
    "    total_labels = len(labels)\n",
    "    correct_predicts = total_labels - wrong_predictions\n",
    "    accuracy = correct_predicts / total_labels\n",
    "    return accuracy\n",
    "    \n",
    "# AUTOLAB_IGNORE_START\n",
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy) # should give 0.956700196554515\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to determine which classifier is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "    classifier = learn_classifier(X, y, kernel)\n",
    "    accuracy = evaluate_classifier(classifier, X, y)\n",
    "    print(kernel,':',accuracy)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there! It's time to write a nice little wrapper function that will use our model to classify unlabeled tweets from tweets_test.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.svm.classes.SVC: classifier learnt\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "\n",
    "    processed_unlabeled_tweets = process_all(unlabeled_tweets)\n",
    "    \n",
    "    list_of_all_words_in_processed_tweets = []\n",
    "    \n",
    "    for row in processed_unlabeled_tweets[\"text\"]:\n",
    "        doc = \"\"\n",
    "        for word in row:\n",
    "            doc = doc + word + \" \"\n",
    "        list_of_all_words_in_processed_tweets.append(doc)\n",
    "    sp = tfidf.transform(list_of_all_words_in_processed_tweets)\n",
    "    predicts = classifier.predict(sp)\n",
    "    return predicts\n",
    "    \n",
    "    \n",
    "# AUTOLAB_IGNORE_START\n",
    "classifier = learn_classifier(X, y, 'linear')\n",
    "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
